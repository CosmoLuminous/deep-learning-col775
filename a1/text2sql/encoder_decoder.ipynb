{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fd1647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 10:13:52.097961: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/lib64\n",
      "2023-03-20 10:13:52.141382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/lib64\n",
      "2023-03-20 10:13:52.141410: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb51f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba8c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length = 7754\n",
      "Encoder Vocab Size = 2040, Decoder Vocab Size = 2342\n"
     ]
    }
   ],
   "source": [
    "class Text2SQLDataset(Dataset):\n",
    "    def __init__(self, file_path, data_prefix = \"train\"):\n",
    "        self.file_path = file_path\n",
    "        self.data = pd.read_excel(os.path.join(file_path, f\"{data_prefix}_data.xlsx\"))\n",
    "        print(\"Dataset Length =\", len(self.data))\n",
    "        with open(os.path.join(file_path, \"encoder.vocab\"), \"r\") as file:\n",
    "            vocab = file.readlines()\n",
    "        self.encoder_vocab = vocab\n",
    "        \n",
    "        with open(os.path.join(file_path, \"decoder.vocab\"), \"r\") as file:\n",
    "            vocab = file.readlines()\n",
    "        self.decoder_vocab = vocab\n",
    "        \n",
    "        with open(os.path.join(file_path, \"encoder_word2idx.pickle\"), \"rb\") as file:\n",
    "            word2idx = pickle.load(file)\n",
    "        with open(os.path.join(file_path, \"encoder_idx2word.pickle\"), \"rb\") as file:\n",
    "            idx2word = pickle.load(file)\n",
    "            \n",
    "        self.en_word2idx = word2idx\n",
    "        self.en_idx2word = idx2word\n",
    "        \n",
    "        with open(os.path.join(file_path, \"decoder_word2idx.pickle\"), \"rb\") as file:\n",
    "            word2idx = pickle.load(file)\n",
    "        with open(os.path.join(file_path, \"decoder_idx2word.pickle\"), \"rb\") as file:\n",
    "            idx2word = pickle.load(file)\n",
    "            \n",
    "        self.de_word2idx = word2idx\n",
    "        self.de_idx2word = idx2word\n",
    "        print(\"Encoder Vocab Size = {}, Decoder Vocab Size = {}\".format(len(self.en_word2idx), len(self.de_word2idx)))\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx, \"\\n\")\n",
    "        try:\n",
    "            query = [\"<sos>\"]\n",
    "            question = [\"<sos>\"]\n",
    "            query = [\"<sos>\"] + tokenize_query(self.data.loc[idx, \"query\"]) + [\"<eos>\"]\n",
    "            question =  [\"<sos>\"] + tokenize_question(self.data.loc[idx, \"question\"]) + [\"<eos>\"]\n",
    "\n",
    "            query = [self.en_word2idx[q] if q in self.en_word2idx else self.en_word2idx[\"<unk>\"] for q in query]\n",
    "            question = [self.en_word2idx[q] if q in self.en_word2idx else self.en_word2idx[\"<unk>\"] for q in question]\n",
    "\n",
    "            sample = {'question': question, 'query': query}\n",
    "        except:\n",
    "            print(idx)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "def collate(batch):\n",
    "    \n",
    "    max_len_ques = max([len(sample['question']) for sample in batch])\n",
    "    max_len_query = max([len(sample['query']) for sample in batch])\n",
    "    \n",
    "    ques_lens = torch.zeros(len(batch), dtype=torch.long)\n",
    "    padded_ques = torch.zeros((len(batch), max_len_ques), dtype=torch.long)\n",
    "    \n",
    "    query_lens = torch.zeros(len(batch), dtype=torch.long)\n",
    "    padded_query = torch.zeros((len(batch), max_len_query), dtype=torch.long)\n",
    "    \n",
    "    for idx in range(len(batch)):\n",
    "        \n",
    "        query = batch[idx]['query']\n",
    "        question = batch[idx]['question']\n",
    "        \n",
    "        ques_len = len(question)\n",
    "        query_len = len(query)\n",
    "        ques_lens[idx] = ques_len\n",
    "        query_lens[idx] = query_len\n",
    "        \n",
    "        padded_ques[idx, :ques_len] = torch.LongTensor(question)\n",
    "        padded_query[idx, :query_len] = torch.LongTensor(query)\n",
    "        \n",
    "    return {'question': padded_ques, 'query': padded_query, 'ques_lens': query_lens, 'query_lens': query_lens}\n",
    "\n",
    "train_dataset = Text2SQLDataset(\"./processed_data/\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1b1be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\", \"<num_value>\", \"<str_value>\"]\n",
    "SQL_KEYWORDS = [\"t\"+str(i+1) for i in range(10)] + [\".\", \",\", \"(\", \")\", \"in\", \"not\", \"and\", \"between\", \"or\", \"where\",\n",
    "            \"except\", \"union\", \"intersect\",\n",
    "            \"group\", \"by\", \"order\", \"limit\", \"having\",\"asc\", \"desc\",\n",
    "            \"count\", \"sum\", \"avg\", \"max\", \"min\",\n",
    "           \"<\", \">\", \"=\", \"!=\", \">=\", \"<=\",\n",
    "            \"like\",\n",
    "            \"distinct\",\"*\",\n",
    "            \"join\", \"on\", \"as\", \"select\", \"from\"\n",
    "           ] \n",
    "SQL_KEYWORDS = dict(zip(SQL_KEYWORDS, [10]*len(SQL_KEYWORDS)))\n",
    "class GloveEmbeddings():\n",
    "    def __init__(self, embed_dim, word2idx):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.word2idx = word2idx\n",
    "        self.special_tokens = SPECIAL_TOKENS\n",
    "        self.vocab_size = len(word2idx)\n",
    "    \n",
    "    def get_embedding_matrix(self):\n",
    "        # Load pre-trained GloVe embeddings\n",
    "        glove = GloVe(name='6B', dim=self.embed_dim)\n",
    "        embedding_matrix = torch.zeros((self.vocab_size, self.embed_dim))\n",
    "\n",
    "        embedding_matrix[0] = torch.zeros(self.embed_dim)    # Padding token\n",
    "        for i in range(1,len(SPECIAL_TOKENS)):            \n",
    "            embedding_matrix[i] = torch.randn(self.embed_dim)    # Start-of-sentence token\n",
    "            \n",
    "        for k, v in self.word2idx.items():\n",
    "            if k in SPECIAL_TOKENS:\n",
    "                continue\n",
    "            else:            \n",
    "                if k in glove.stoi:\n",
    "                    embedding_matrix[v] = torch.tensor(glove.vectors[glove.stoi[k]])\n",
    "                else:\n",
    "                    embedding_matrix[v] = embedding_matrix[1]\n",
    "#                     print(\"unknown token\", v)\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_units=1024, num_layers=1, p = 0.5, bidirectional=False, embed_matrix=None):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embed_matrix = None\n",
    "        if self.embed_matrix is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, padding_idx=0)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_size, self.embed_dim, padding_idx=0)\n",
    "        self.LSTM = nn.LSTM(embed_dim, hidden_units, num_layers = num_layers, dropout=p, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(\"ENCODER INPUT SHAPE\", x.shape)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "#         print(\"ENCODER EMBEDDING SHAPE\", x.shape)\n",
    "        \n",
    "        encoder_out, (ht, ct) = self.LSTM(x)        \n",
    "#         print(\"ENCODER OUTPUT SHAPE: encoder_out, ht, ct\", encoder_out.shape, ht.shape, ct.shape)\n",
    "        \n",
    "        return encoder_out, (ht, ct)\n",
    "    \n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_units=1024, num_layers=1, p = 0.5):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, self.embed_dim, padding_idx=0)\n",
    "        self.LSTM = nn.LSTM(embed_dim, hidden_units, num_layers = num_layers, dropout=p, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, input_size)\n",
    "        \n",
    "    def forward(self, x, h0_c0):\n",
    "#         print(\"|== Decoder Input Shape: x, h0_c0\", x.shape, len(h0_c0), h0_c0[0].shape, h0_c0[1].shape)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "#         print(\"|== Decoder Embeddings Shape: x\", x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "#         print(\"|== Decoder Embeddings unsqueezed(0) Shape: x\", x.shape)\n",
    "        decoder_out, (ht, ct) = self.LSTM(x, h0_c0)\n",
    "#         print(\"|== Decoder Output Shape Shape: decoder_out, ht, ct\", decoder_out.shape, ht.shape, ct.shape)\n",
    "        \n",
    "        out = self.fc(decoder_out)\n",
    "#         print(\"|== Decoder FC OUT Shape: out\", out.shape)\n",
    "        \n",
    "        return out, (ht, ct)\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.args = args\n",
    "        self.model_type = args.model_type\n",
    "        self.embed_dim = args.embed_dim        \n",
    "        self.encoder_hidden_units = args.en_hidden\n",
    "        self.decoder_hidden_units = args.de_hidden\n",
    "        self.encoder_num_layers = args.en_num_layers\n",
    "        self.decoder_num_layers = args.de_num_layers\n",
    "        self.processed_data = args.processed_data\n",
    "        self.encoder_word2idx = self.get_encoder_word2idx()\n",
    "        self.decoder_word2idx = self.get_decoder_word2idx()\n",
    "        self.encoder_input_size = len(self.encoder_word2idx)\n",
    "        self.decoder_input_size = len(self.decoder_word2idx)\n",
    "        self.encoder = self.get_encoder()\n",
    "        self.decoder = self.get_decoder()\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_encoder_word2idx(self):\n",
    "        with open(os.path.join(self.processed_data, \"encoder_word2idx.pickle\"), \"rb\") as file:\n",
    "            word2idx = pickle.load(file)\n",
    "        with open(os.path.join(self.processed_data, \"encoder_idx2word.pickle\"), \"rb\") as file:\n",
    "            idx2word = pickle.load(file)\n",
    "            \n",
    "#         self.en_word2idx = word2idx\n",
    "#         self.en_idx2word = idx2word        \n",
    "        \n",
    "        return word2idx\n",
    "    \n",
    "    def get_decoder_word2idx(self):\n",
    "        \n",
    "        with open(os.path.join(self.processed_data, \"decoder_word2idx.pickle\"), \"rb\") as file:\n",
    "            word2idx = pickle.load(file)\n",
    "        with open(os.path.join(self.processed_data, \"decoder_idx2word.pickle\"), \"rb\") as file:\n",
    "            idx2word = pickle.load(file)\n",
    "            \n",
    "#         self.de_word2idx = word2idx\n",
    "#         self.de_idx2word = idx2word\n",
    "        \n",
    "        return word2idx\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        print(\"Loading GloVe embeddings...\")\n",
    "        glove = GloveEmbeddings(self.embed_dim, self.encoder_word2idx)\n",
    "        embedding_matrix = glove.get_embedding_matrix()\n",
    "        print(\"Loading Encoder...\")\n",
    "        encoder = LSTMEncoder(input_size = self.encoder_input_size, embed_dim = self.embed_dim, \n",
    "                              hidden_units=self.encoder_hidden_units, num_layers=self.encoder_num_layers, p = 0.3, bidirectional=False, embed_matrix=embedding_matrix)\n",
    "        \n",
    "        return encoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        \n",
    "        if self.model_type == \"Seq2Seq\":\n",
    "            print(\"Loading Seq2Seq LSTM Decoder...\")\n",
    "            decoder = LSTMDecoder(input_size = self.decoder_input_size, embed_dim = self.embed_dim, \n",
    "                              hidden_units=self.decoder_hidden_units, num_layers=self.decoder_num_layers, p = 0.3)\n",
    "        \n",
    "        elif self.model_type == \"Seq2SeqAttn\":\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        return decoder\n",
    "        \n",
    "    def forward(self, question, query, tf_ratio=0.5):\n",
    "        batch_size = question.shape[0]\n",
    "        target_len = query.shape[1]\n",
    "        \n",
    "        _, (hidden, cell) = self.encoder(question)\n",
    "        \n",
    "        target_vocab_size = self.decoder_input_size\n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n",
    "        \n",
    "        x = query[:,0]\n",
    "        for t in range(1, target_len):\n",
    "            output, (hidden, cell) = self.decoder(x, (hidden, cell))\n",
    "#             print(\"Seq2seq out shape\", output.shape)\n",
    "            output = output.squeeze(1)\n",
    "            outputs[:,t,:] = output\n",
    "            x = output.argmax(dim=1)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96386f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Loading GloVe embeddings...\n",
      "Loading Encoder...\n",
      "Loading Seq2Seq LSTM Decoder...\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtech/siy197580/.local/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.7632012367248535\n",
      "None\n",
      "2.829655170440674\n",
      "None\n",
      "2.369840145111084\n",
      "None\n",
      "2.7263875007629395\n",
      "None\n",
      "2.661227226257324\n",
      "None\n",
      "Epoch: 1\n",
      "2.6697356700897217\n",
      "None\n",
      "2.628690004348755\n",
      "None\n",
      "2.8142495155334473\n",
      "None\n",
      "2.4986259937286377\n",
      "None\n",
      "2.4327619075775146\n",
      "None\n",
      "Epoch: 2\n",
      "2.422717332839966\n",
      "None\n",
      "2.607929229736328\n",
      "None\n",
      "2.367811918258667\n",
      "None\n",
      "2.3216230869293213\n",
      "None\n",
      "2.4766297340393066\n",
      "None\n",
      "Epoch: 3\n",
      "2.3596885204315186\n",
      "None\n",
      "2.4122962951660156\n",
      "None\n",
      "2.5834128856658936\n",
      "None\n",
      "2.5238888263702393\n",
      "None\n",
      "2.4882805347442627\n",
      "None\n",
      "Epoch: 4\n",
      "2.20483136177063\n",
      "None\n",
      "2.545891284942627\n",
      "None\n",
      "2.416046619415283\n",
      "None\n",
      "2.2803456783294678\n",
      "None\n",
      "2.310915470123291\n",
      "None\n",
      "Epoch: 5\n",
      "2.8579797744750977\n",
      "None\n",
      "2.047302007675171\n",
      "None\n",
      "2.1798903942108154\n",
      "None\n",
      "2.186638116836548\n",
      "None\n",
      "2.6594316959381104\n",
      "None\n",
      "Epoch: 6\n",
      "2.064645290374756\n",
      "None\n",
      "2.768460988998413\n",
      "None\n",
      "2.412048816680908\n",
      "None\n",
      "2.1769721508026123\n",
      "None\n",
      "2.1773533821105957\n",
      "None\n",
      "Epoch: 7\n",
      "2.028174877166748\n",
      "None\n",
      "2.273200273513794\n",
      "None\n",
      "1.9562876224517822\n",
      "None\n",
      "1.6715636253356934\n",
      "None\n",
      "2.1585137844085693\n",
      "None\n",
      "Epoch: 8\n",
      "1.8325828313827515\n",
      "None\n",
      "1.8308302164077759\n",
      "None\n",
      "1.7038097381591797\n",
      "None\n",
      "1.9877361059188843\n",
      "None\n",
      "2.492288112640381\n",
      "None\n",
      "Epoch: 9\n",
      "1.687695026397705\n",
      "None\n",
      "2.4620535373687744\n",
      "None\n",
      "2.0078516006469727\n",
      "None\n",
      "1.8851970434188843\n",
      "None\n",
      "2.0139057636260986\n",
      "None\n",
      "Epoch: 10\n",
      "1.7548173666000366\n",
      "None\n",
      "1.630713701248169\n",
      "None\n",
      "1.840254306793213\n",
      "None\n",
      "1.6975065469741821\n",
      "None\n",
      "2.105175495147705\n",
      "None\n",
      "Epoch: 11\n",
      "1.9238425493240356\n",
      "None\n",
      "2.4658286571502686\n",
      "None\n",
      "2.013436794281006\n",
      "None\n",
      "1.8054732084274292\n",
      "None\n",
      "1.7936404943466187\n",
      "None\n",
      "Epoch: 12\n",
      "1.9767733812332153\n",
      "None\n",
      "2.083588123321533\n",
      "None\n",
      "1.9520541429519653\n",
      "None\n",
      "1.6801633834838867\n",
      "None\n",
      "1.9194811582565308\n",
      "None\n",
      "Epoch: 13\n",
      "1.9840930700302124\n",
      "None\n",
      "1.5559308528900146\n",
      "None\n",
      "1.573060154914856\n",
      "None\n",
      "1.4317750930786133\n",
      "None\n",
      "1.7652289867401123\n",
      "None\n",
      "Epoch: 14\n",
      "1.59315824508667\n",
      "None\n",
      "1.417311668395996\n",
      "None\n",
      "1.6021606922149658\n",
      "None\n",
      "1.4700430631637573\n",
      "None\n",
      "1.5904544591903687\n",
      "None\n",
      "Epoch: 15\n",
      "1.7916618585586548\n",
      "None\n",
      "1.7283780574798584\n",
      "None\n",
      "1.698285698890686\n",
      "None\n",
      "1.6429839134216309\n",
      "None\n",
      "1.9108136892318726\n",
      "None\n",
      "Epoch: 16\n",
      "1.4476467370986938\n",
      "None\n",
      "1.6523141860961914\n",
      "None\n",
      "1.4391998052597046\n",
      "None\n",
      "1.4683291912078857\n",
      "None\n",
      "1.641883134841919\n",
      "None\n",
      "Epoch: 17\n",
      "2.2593259811401367\n",
      "None\n",
      "2.096088171005249\n",
      "None\n",
      "1.7728568315505981\n",
      "None\n",
      "1.4674510955810547\n",
      "None\n",
      "1.537230134010315\n",
      "None\n",
      "Epoch: 18\n",
      "1.5569615364074707\n",
      "None\n",
      "1.416378140449524\n",
      "None\n",
      "1.7812306880950928\n",
      "None\n",
      "1.1636080741882324\n",
      "None\n",
      "1.3804469108581543\n",
      "None\n",
      "Epoch: 19\n",
      "1.291698694229126\n",
      "None\n",
      "1.5603795051574707\n",
      "None\n",
      "1.5265065431594849\n",
      "None\n",
      "1.491043210029602\n",
      "None\n",
      "1.2557227611541748\n",
      "None\n",
      "Epoch: 20\n",
      "1.3924040794372559\n",
      "None\n",
      "1.3591641187667847\n",
      "None\n",
      "1.9321385622024536\n",
      "None\n",
      "1.2364284992218018\n",
      "None\n",
      "1.551334023475647\n",
      "None\n",
      "Epoch: 21\n",
      "1.7956578731536865\n",
      "None\n",
      "1.9442687034606934\n",
      "None\n",
      "1.5967612266540527\n",
      "None\n",
      "1.7149332761764526\n",
      "None\n",
      "1.179802417755127\n",
      "None\n",
      "Epoch: 22\n",
      "1.8838870525360107\n",
      "None\n",
      "1.0771634578704834\n",
      "None\n",
      "1.7284373044967651\n",
      "None\n",
      "1.4702242612838745\n",
      "None\n",
      "1.1531565189361572\n",
      "None\n",
      "Epoch: 23\n",
      "1.6675535440444946\n",
      "None\n",
      "1.5142050981521606\n",
      "None\n",
      "1.5277661085128784\n",
      "None\n",
      "1.2156463861465454\n",
      "None\n",
      "1.2927587032318115\n",
      "None\n",
      "Epoch: 24\n",
      "1.4534313678741455\n",
      "None\n",
      "1.545812964439392\n",
      "None\n",
      "2.0411219596862793\n",
      "None\n",
      "1.1828696727752686\n",
      "None\n",
      "1.5238707065582275\n",
      "None\n",
      "Epoch: 25\n",
      "1.2317817211151123\n",
      "None\n",
      "1.2620623111724854\n",
      "None\n",
      "1.3536714315414429\n",
      "None\n",
      "1.3661072254180908\n",
      "None\n",
      "1.599999189376831\n",
      "None\n",
      "Epoch: 26\n",
      "1.649263620376587\n",
      "None\n",
      "1.397186517715454\n",
      "None\n",
      "1.219957947731018\n",
      "None\n",
      "1.2522677183151245\n",
      "None\n",
      "1.619195818901062\n",
      "None\n",
      "Epoch: 27\n",
      "0.9328591823577881\n",
      "None\n",
      "1.7110151052474976\n",
      "None\n",
      "1.5405594110488892\n",
      "None\n",
      "1.6429206132888794\n",
      "None\n",
      "1.1684718132019043\n",
      "None\n",
      "Epoch: 28\n",
      "1.08572518825531\n",
      "None\n",
      "1.2639715671539307\n",
      "None\n",
      "1.357593059539795\n",
      "None\n",
      "1.6475262641906738\n",
      "None\n",
      "1.2608485221862793\n",
      "None\n",
      "Epoch: 29\n",
      "1.465053677558899\n",
      "None\n",
      "1.57445228099823\n",
      "None\n",
      "1.151415228843689\n",
      "None\n",
      "1.468831181526184\n",
      "None\n",
      "1.1841038465499878\n",
      "None\n",
      "Epoch: 30\n",
      "1.3797587156295776\n",
      "None\n",
      "1.1602518558502197\n",
      "None\n",
      "1.021250605583191\n",
      "None\n",
      "1.162585735321045\n",
      "None\n",
      "1.4576336145401\n",
      "None\n",
      "Epoch: 31\n",
      "1.2853909730911255\n",
      "None\n",
      "1.3202673196792603\n",
      "None\n",
      "1.2855929136276245\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = Seq2Seq(args).to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "schedulers = [\n",
    "        optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1, last_epoch=- 1, verbose=False),\n",
    "        optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, verbose=False)\n",
    "        ]\n",
    "scheduler =  schedulers[1]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle=True, \n",
    "                          num_workers=args.num_workers, collate_fn=collate)\n",
    "for epoch in range(args.epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for i, data in enumerate(train_loader):\n",
    "    #     print(data['question'].shape, data['query'].shape, data['ques_lens'].shape, data['query_lens'].shape)\n",
    "        optimizer.zero_grad()\n",
    "        question = data['question'].to(device)\n",
    "        query = data['query'].to(device)\n",
    "        output = model(question, query)\n",
    "    #     print(\"output and target\", output.shape, query.shape)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        query = query.reshape(-1)    \n",
    "    #     print(\"reshaped output and target\", output.shape, query.shape)\n",
    "        loss = criterion(output, query)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(loss.item())\n",
    "    scheduler.step()\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "090f018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 20 10:11:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000     Off  | 00000000:C1:00.0 Off |                  Off |\n",
      "| 33%   29C    P8    16W / 230W |   1375MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     36653      C   ...0/anaconda3/bin/python3.7     1259MiB |\n",
      "|    0   N/A  N/A     41115      C   ...ffice/program/soffice.bin      113MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0feb5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    def __init__(self):\n",
    "        self.model_type = \"Seq2Seq\"\n",
    "        self.data_dir = \"./data\"\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 8\n",
    "        self.epochs = 100\n",
    "        self.en_hidden = 512\n",
    "        self.de_hidden = 512\n",
    "        self.en_num_layers = 1\n",
    "        self.de_num_layers = 1\n",
    "        self.embed_dim = 300\n",
    "        self.processed_data = \"./processed_data/\"\n",
    "        \n",
    "args = ARGS()\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"\n",
    "    Generate a parameter parser\n",
    "    \"\"\"\n",
    "    # parse parameters\n",
    "    parser = argparse.ArgumentParser(description=\"Text2SQL\")\n",
    "    \n",
    "    # model type\n",
    "    parser.add_argument(\"--model_type\", type=str, default=\"Seq2Seq\", help=\"Select the model you want to run from [Seq2Seq, Seq2SeqAttn].\")\n",
    "\n",
    "    # path to data files.\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"./data\", help=\"Path to dataset directory.\")\n",
    "\n",
    "    # path to result files.\n",
    "    parser.add_argument(\"--result_dir\", type=str, default=\"./results\", help=\"Path to dataset directory.\")\n",
    "\n",
    "    # path to model checkpoints.\n",
    "    parser.add_argument(\"--checkpoint_dir\", type=str, default=\"./checkpoints\", help=\"Path to model checkpoints.\")\n",
    "    \n",
    "    # path to model checkpoints.\n",
    "    parser.add_argument(\"--processed_data\", type=str, default=\"./processed_data\", help=\"Path to processed data.\")\n",
    "\n",
    "    # batch size training\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size to be used during training.\")\n",
    "\n",
    "    # number of workers for dataloader\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8, help=\"Number of workers used for dataloading.\")\n",
    "\n",
    "    # max number of epochs\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100, help=\"Number of workers used for dataloading.\")\n",
    "\n",
    "    parser.add_argument(\"--en_hidden\", type=int, default=512, help=\"Encoder Hidden Units\")\n",
    "    \n",
    "    parser.add_argument(\"--de_hidden\", type=int, default=512, help=\"Decoder Hidden Units\")\n",
    "\n",
    "    parser.add_argument(\"--en_num_layers\", type=int, default=1, help=\"Number of lstm layers in encoder.\")\n",
    "    \n",
    "    parser.add_argument(\"--de_num_layers\", type=int, default=1, help=\"Number of lstm layers in decoder.\")    \n",
    "    \n",
    "    parser.add_argument(\"--embed_dim\", type=int, default=300, help=\"Embeddings dimension for both encoder and decoder.\")\n",
    "\n",
    "    return parser\n",
    "# parser = get_parser()\n",
    "# args = parser.parse_args()\n",
    "# args.data_dir = os.path.relpath(args.data_dir)\n",
    "# print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9b56f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seq2Seq'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8e9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
