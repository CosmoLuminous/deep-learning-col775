{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a680b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca08b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\", \"<num_value>\", \"<str_value>\"]\n",
    "\n",
    "class GloveEmbeddings():\n",
    "    def __init__(self, embed_dim, word2idx):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.word2idx = word2idx\n",
    "        seld.idx2word = idx2word\n",
    "        self.special_tokens = SPECIAL_TOKENS\n",
    "        self.vocab_size = len(word2idx)\n",
    "    \n",
    "    def get_embedding_matrix(self):\n",
    "        # Load pre-trained GloVe embeddings\n",
    "        glove = GloVe(name='6B', dim=self.embed_dim)\n",
    "        embedding_matrix = torch.zeros((self.vocab_size, self.embed_dim))\n",
    "\n",
    "        embedding_matrix[0] = torch.zeros(self.embed_dim)    # Padding token\n",
    "        for i in range(1,len(SPECIAL_TOKENS)):            \n",
    "            embedding_matrix[i] = torch.randn(self.embed_dim)    # Start-of-sentence token\n",
    "            \n",
    "        for k, v in self.word2idx.items():\n",
    "            if k in SPECIAL_TOKENS:\n",
    "                continue\n",
    "            else:            \n",
    "                if k in glove.stoi:\n",
    "                    embedding_matrix[v] = torch.tensor(glove.vectors[glove.stoi[k]])\n",
    "                else:\n",
    "                    embedding_matrix[v] = embedding_matrix[1]\n",
    "                    print(\"unknown token\", v)\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_units=1024, num_layers=1, p = 0.5, bidirectional=False, embed_matrix=None):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropour(p)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embed_matrix = None\n",
    "        if self.embed_matrix in not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, padding_idx=0)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_size, self.embed_dim, padding_idx=0)\n",
    "        self.LSTM = nn.LSTM(embed_dim, hidden_size, num_layers = num_layers, dropout=p, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        \n",
    "        x_encoder, (ht, ct) = self.lstm(x)\n",
    "        \n",
    "        return x_encoder, (ht, ct)\n",
    "    \n",
    "self LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_units=1024, num_layers=1, p = 0.5):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropour(p)\n",
    "        self.embedding = nn.Embedding(input_size, self.embed_dim, padding_idx=0)\n",
    "        self.LSTM = nn.LSTM(embed_dim, hidden_size, num_layers = num_layers, dropout=p, batch_first=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f869a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2SQLDataset(Dataset):\n",
    "    def __init__(self, file_path, vocab_path, data_prefix = \"train\"):\n",
    "        self.file_path = file_path\n",
    "        self.data = pd.read_excel(os.path.join(file_path, f\"{data_prefix}_data.xlsx\"))\n",
    "        print(\"Dataset Length =\", len(self.data))\n",
    "        with open(os.path.join(vocab_path, \"train.vocab\"), \"r\") as file:\n",
    "            vocab = file.readlines()\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        with open(os.path.join(vocab_path, \"word2idx.pickle\"), \"rb\") as file:\n",
    "            word2idx = pickle.load(file)\n",
    "        with open(os.path.join(vocab_path, \"idx2word.pickle\"), \"rb\") as file:\n",
    "            idx2word = pickle.load(file)\n",
    "            \n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx, \"\\n\")\n",
    "        query = [\"<sos>\"]\n",
    "        question = [\"<sos>\"]\n",
    "        query = [\"<sos>\"] + tokenize_query(self.data.loc[idx, \"query\"]) + [\"<eos>\"]\n",
    "        question =  [\"<sos>\"] + tokenize_query(self.data.loc[idx, \"question\"]) + [\"<eos>\"]\n",
    "        \n",
    "        query = [self.word2idx[q] if q in self.word2idx else self.word2idx[\"<unk>\"] for q in query]\n",
    "        question = [self.word2idx[q] if q in self.word2idx else self.word2idx[\"<unk>\"] for q in question]\n",
    "        \n",
    "        sample = {'question': question, 'query': query}\n",
    "        return sample\n",
    "    \n",
    "def collate(batch):\n",
    "    \n",
    "    max_len_ques = max([len(sample['question']) for sample in batch])\n",
    "    max_len_query = max([len(sample['query']) for sample in batch])\n",
    "    \n",
    "    ques_lens = torch.zeros(len(batch), dtype=torch.long)\n",
    "    padded_ques = torch.zeros((len(batch), max_len_ques), dtype=torch.long)\n",
    "    \n",
    "    query_lens = torch.zeros(len(batch), dtype=torch.long)\n",
    "    padded_query = torch.zeros((len(batch), max_len_query), dtype=torch.long)\n",
    "    \n",
    "    for idx in range(len(batch)):\n",
    "        \n",
    "        query = batch[idx]['query']\n",
    "        question = batch[idx]['question']\n",
    "        \n",
    "        ques_len = len(question)\n",
    "        query_len = len(query)\n",
    "        ques_lens[idx] = ques_len\n",
    "        query_lens[idx] = query_len\n",
    "        \n",
    "        padded_ques[idx, :ques_len] = torch.LongTensor(question)\n",
    "        padded_query[idx, :query_len] = torch.LongTensor(query)\n",
    "        \n",
    "    return {'question': padded_ques, 'query': padded_query, 'ques_lens': query_lens, 'query_lens': query_lens}\n",
    "\n",
    "train_dataset = Text2SQLDataset(\"./intermediate_files/\", \"./intermediate_files/\", \"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size = 2, shuffle=True, num_workers=1, collate_fn=collate)\n",
    "for i, data in enumerate(train_loader):\n",
    "    print(data['question'].shape, data['query'].shape, data['ques_lens'].shape, data['query_lens'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130d5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "799a9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/tables.json\", \"r\") as file:\n",
    "    tables = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21f30ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "def tokenize_query(query):\n",
    "    \"\"\"WARNING: THIS IS A VERY NAIVE TOKENIZER. IMPROVE THIS LATER\"\"\"\n",
    "    \n",
    "    query_tokens = word_tokenize(query)\n",
    "    \n",
    "    \n",
    "    return query_tokens\n",
    "\n",
    "def tokenize_question(question):\n",
    "    \"\"\"WARNING: THIS IS A VERY NAIVE TOKENIZER. IMPROVE THIS LATER\"\"\"\n",
    "    \n",
    "    ques_tokens = word_tokenize(question)    \n",
    "    \n",
    "    return ques_tokens\n",
    "\n",
    "def generate_schema_vocab(file_path):\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        schemas = json.load(f)\n",
    "        \n",
    "    databases = set()\n",
    "    tokens_db_lookup = defaultdict(set)\n",
    "    schema_vocab = Counter()\n",
    "    \n",
    "    for schema in schemas:\n",
    "        db_id = schema[\"db_id\"]\n",
    "        schema_tokens = []\n",
    "        \n",
    "        if db_id not in databases:\n",
    "            databases.add(db_id)\n",
    "        \n",
    "        for column in schema[\"column_names_original\"]:\n",
    "            schema_tokens.append(column[1].lower())\n",
    "        \n",
    "        for table in schema[\"table_names_original\"]:\n",
    "            schema_tokens.append(table.lower())\n",
    "        \n",
    "        for token in list(Counter(schema_tokens).keys()):\n",
    "            tokens_db_lookup[token].add(db_id)\n",
    "        \n",
    "        schema_vocab.update(schema_tokens)    \n",
    "    \n",
    "    return schema_vocab, databases, tokens_db_lookup\n",
    "\n",
    "\n",
    "\n",
    "def generate_query_question_vocab(file_path, output_path, file_type=\"train\", save=False):\n",
    "    ques_vocab = Counter()\n",
    "    query_vocab = Counter()\n",
    "    \n",
    "    \n",
    "#     with open(file_path, \"r\") as f:\n",
    "#         data_file = json.load(f)\n",
    "\n",
    "    data_file = pd.read_csv(file_path)\n",
    "    \n",
    "    if save:\n",
    "        ques_outfile = open(os.path.join(output_path, f\"{file_type}_questions.txt\"), \"w\")\n",
    "        query_outfile = open(os.path.join(output_path, f\"{file_type}_query.txt\"), \"w\")\n",
    "        query_db_outfile = open(os.path.join(output_path, f\"{file_type}_query_db.txt\"), \"w\")\n",
    "        \n",
    "    \n",
    "    for idx, dp in data_file.iterrows():\n",
    "        question = dp[\"question\"]\n",
    "        query = dp[\"query\"]\n",
    "        db_id = dp[\"db_id\"]\n",
    "        ques_tokens = tokenize_question(question)\n",
    "        query_tokens = tokenize_query(query)\n",
    "        \n",
    "        ques_vocab.update(ques_tokens)\n",
    "        query_vocab.update(query_tokens)\n",
    "        \n",
    "        ques_sentence = \" \".join(ques_tokens)\n",
    "        query_sentence = \" \".join(query_tokens)\n",
    "        \n",
    "        if save:\n",
    "            try:\n",
    "                ques_outfile.write(f\"{ques_sentence}\\n\")\n",
    "            except:\n",
    "                ques_outfile.write(f\"{ques_sentence.encode('utf-8')}\\n\")\n",
    "            \n",
    "            try:\n",
    "                query_outfile.write(f\"{query_sentence}\\n\")\n",
    "            except:\n",
    "                query_outfile.write(f\"{query_sentence.encode('utf-8')}\\n\")\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                q = query.lower().replace('\\t', ' ')\n",
    "                query_db_outfile.write(\"{}\\t{}\\n\".format(q, db_id))\n",
    "            except:\n",
    "                q = query.encode('utf-8').lower().replace('\\t', ' ')\n",
    "                query_db_outfile.write(\"{}\\t{}\\n\".format(q, db_id))\n",
    "            \n",
    "    \n",
    "    if save:\n",
    "        ques_outfile.close()\n",
    "        query_outfile.close()\n",
    "        query_db_outfile.close()\n",
    "        \n",
    "    return ques_vocab, query_vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c225560",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, db, lookup = generate_schema_vocab(\"./data/tables.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c1a1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_vocab, query_vocab = generate_query_question_vocab(\"./data/train.csv\", \"./intermediate_files/\", \"train\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ef6cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 if False else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6f29cdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `\"punkt\"'\n",
      "/bin/bash: -c: line 0: `nltk.download(\"punkt\")'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "!nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d1d3109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `'punkt''\n",
      "/bin/bash: -c: line 0: `nltk.download('punkt')'\n"
     ]
    }
   ],
   "source": [
    "!nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d772538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['column_names', 'column_names_original', 'column_types', 'db_id', 'foreign_keys', 'primary_keys', 'table_names', 'table_names_original'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71a76374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classroom',\n",
       " 'department',\n",
       " 'course',\n",
       " 'instructor',\n",
       " 'section',\n",
       " 'teaches',\n",
       " 'student',\n",
       " 'takes',\n",
       " 'advisor',\n",
       " 'time_slot',\n",
       " 'prereq']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[1]['table_names_original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f370cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Counter(['classroom',\n",
    " 'department',\n",
    " 'course',\n",
    " 'instructor',\n",
    " 'section',\n",
    " 'teaches',\n",
    " 'student',\n",
    " 'takes',\n",
    " 'advisor',\n",
    " 'time_slot',\n",
    " 'prereq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fdcd534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['classroom', 'department', 'course', 'instructor', 'section', 'teaches', 'student', 'takes', 'advisor', 'time_slot', 'prereq'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33e12fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "l = [\".\", \",\", \"(\", \")\", \"in\", \"not\", \"and\", \"between\", \"or\", \"where\",\n",
    "            \"except\", \"union\", \"intersect\",\n",
    "            \"group\", \"by\", \"order\", \"limit\", \"having\",\"asc\", \"desc\",\n",
    "            \"count\", \"sum\", \"avg\", \"max\", \"min\",\n",
    "           \"<\", \">\", \"=\", \"!=\", \">=\", \"<=\",\n",
    "            \"like\",\n",
    "            \"distinct\",\"*\",\n",
    "            \"join\", \"on\", \"as\", \"select\", \"from\"\n",
    "           ] + [\"t\"+str(i+1) for i in range(10)]\n",
    "d = dict()\n",
    "\n",
    "for i in l:\n",
    "    d[i] = 10\n",
    "\n",
    "with open('./vocab_data/sql_keywords.pickle', 'wb') as file:\n",
    "    pickle.dump(d, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89e9051c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 10,\n",
       " ',': 10,\n",
       " '(': 10,\n",
       " ')': 10,\n",
       " 'in': 10,\n",
       " 'not': 10,\n",
       " 'and': 10,\n",
       " 'between': 10,\n",
       " 'or': 10,\n",
       " 'where': 10,\n",
       " 'except': 10,\n",
       " 'union': 10,\n",
       " 'intersect': 10,\n",
       " 'group': 10,\n",
       " 'by': 10,\n",
       " 'order': 10,\n",
       " 'limit': 10,\n",
       " 'having': 10,\n",
       " 'asc': 10,\n",
       " 'desc': 10,\n",
       " 'count': 10,\n",
       " 'sum': 10,\n",
       " 'avg': 10,\n",
       " 'max': 10,\n",
       " 'min': 10,\n",
       " '<': 10,\n",
       " '>': 10,\n",
       " '=': 10,\n",
       " '!=': 10,\n",
       " '>=': 10,\n",
       " '<=': 10,\n",
       " 'like': 10,\n",
       " 'distinct': 10,\n",
       " '*': 10,\n",
       " 'join': 10,\n",
       " 'on': 10,\n",
       " 'as': 10,\n",
       " 'select': 10,\n",
       " 'from': 10,\n",
       " 't1': 10,\n",
       " 't2': 10,\n",
       " 't3': 10,\n",
       " 't4': 10,\n",
       " 't5': 10,\n",
       " 't6': 10,\n",
       " 't7': 10,\n",
       " 't8': 10,\n",
       " 't9': 10,\n",
       " 't10': 10}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join('./vocab_data/sql_keywords.pickle'), 'rb') as file:\n",
    "    a = pickle.load(file)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b66327d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"{unk}\", \"{sos}\", \"{eos}\", \"{value}\"]\n",
    "SPL_TOKENS_TO_IDX = {SPECIAL_TOKENS[v]: v for v in range(len(SPECIAL_TOKENS))}\n",
    "SPL_IDX_TO_TOKENS = {v: SPECIAL_TOKENS[v] for v in range(len(SPECIAL_TOKENS))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b86071a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'{unk}': 0, '{sos}': 1, '{eos}': 2, '{value}': 3}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPL_TOKENS_TO_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "848a1024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '{unk}', 1: '{sos}', 2: '{eos}', 3: '{value}'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPL_IDX_TO_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a41f53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38b296d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2, 2: 2, 3: 2, 4: 4, 5: 2, 9: 1})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr = Counter([1,2,3,4,4,4,5,5,3,2,1,4,9])\n",
    "ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1dfd7dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1, 2: 2, 4: 1, 3: 3, 5: 1, 6: 1})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr1 = Counter([1,2,4,3,2,5,3,6,3])\n",
    "ctr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "31630215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 3, 2: 4, 3: 5, 4: 5, 5: 3, 9: 1, 6: 1})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr.update(ctr1)\n",
    "ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9016dee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 5, 4: 5, 2: 4, 1: 3}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(ctr.most_common(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1d92ea74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '12', '2', '23']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r'[-+]?\\b\\d+\\b', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "eaed5780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a<STR>asd <NUM> , <NUM> , '<NUM>', '<STR>', T4.abc\""
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s = \"adfasd 1 , 12.2 , '23', 'df', T4.abc\"\n",
    "regex_nums = \"[-+]?\\d*\\.\\d+\"\n",
    "nums = re.findall(regex_nums, s)\n",
    "for v in nums:\n",
    "    s = s.replace(v, \"<NUM>\")\n",
    "\n",
    "regex_nums = r'[-+]?\\b\\d+\\b'\n",
    "nums = re.findall(regex_nums, s)\n",
    "for v in nums:\n",
    "    s = s.replace(v, \"<NUM>\")\n",
    "    \n",
    "regex_str1 = r\"'([A-Za-z_\\./\\\\-]*)'\" #\"\\\"[^\\\"]*\\\"\"\n",
    "str1 = re.findall(regex_str1, s)\n",
    "for v in str1:\n",
    "    s = s.replace(v, \"<STR>\")\n",
    "    \n",
    "regex_str2 = r'\"([A-Za-z_\\./\\\\-]*)\"' #\"\\'[^\\']*\\'\"\n",
    "str2 = re.findall(regex_str2, s)\n",
    "for v in str2:\n",
    "    s = s.replace(v, \"<STR>\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "088ad781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'23'\", \"'df'\"]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"adfasd 1 , 12.2 , '23', 'df', T4.abc\"\n",
    "regex_nums = \"\\'[^\\']*\\'\" \n",
    "re.findall(regex_nums, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "de3cab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.update([2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "add95a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'abv\", \"'\"]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"'abv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "79ad4afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"A\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5008f46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1] + [2,3,4] + [5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e84ef391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:35<00:00, 11423.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "glove = GloVe(name='6B', dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f4ce6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0380e-01,  1.8126e-01,  4.6583e-01, -6.6440e-01, -4.4070e-01,\n",
       "         1.7174e-01, -5.0796e-01, -4.2103e-01,  1.6000e-01,  6.5258e-01,\n",
       "        -5.7537e-01,  3.7265e-01,  6.9735e-01,  7.1328e-01,  1.7069e-01,\n",
       "         4.0841e-01, -6.1980e-01,  5.2908e-01,  1.1537e-01,  2.0981e-01,\n",
       "         5.6525e-01,  2.9440e+00,  7.0009e-01, -1.8037e-01,  1.0374e-01,\n",
       "        -4.3081e-01, -1.3472e-02,  1.5318e-01, -5.7869e-01, -3.2528e-01,\n",
       "        -7.2414e-01, -1.4693e-01,  1.3082e-01, -4.4664e-01, -5.2502e-01,\n",
       "         2.5720e-01, -2.1991e-01, -6.1173e-02, -1.5098e-01,  2.5422e-01,\n",
       "        -3.6608e-01,  3.5592e-01, -3.4717e-01,  5.6783e-01, -3.9235e-01,\n",
       "         4.1060e-01,  5.7588e-01,  4.0124e-02, -5.8766e-02,  4.0908e-01,\n",
       "         2.6878e-01, -1.2518e-01,  1.8262e-01,  8.3374e-02,  2.3665e-01,\n",
       "        -2.9179e-01,  4.0927e-01, -3.1596e-01, -1.2123e-01, -1.2644e-01,\n",
       "         2.1737e-01, -4.0186e-01, -7.3033e-01, -1.1869e-01, -7.8917e-01,\n",
       "        -5.7036e-02, -4.6895e-01,  6.6060e-02,  5.5962e-01, -1.1137e-01,\n",
       "         7.8175e-01, -3.7551e-01,  2.2163e-02, -2.4323e-01, -3.2796e-01,\n",
       "        -2.9892e-01, -1.2447e+00, -1.9143e-01, -2.1058e-01,  2.2987e-01,\n",
       "         1.1876e-02,  2.4012e-01, -6.5169e-01, -2.3992e-01,  4.9102e-01,\n",
       "        -5.8087e-01, -3.8480e-01, -6.2579e-01,  5.0061e-01, -1.5484e+00,\n",
       "         4.4714e-01, -1.8681e-02,  2.8949e-02,  2.8168e-01, -1.6786e-01,\n",
       "        -2.9398e-01,  4.3597e-02,  1.3934e-01, -3.9626e-01,  9.4014e-02,\n",
       "        -1.2166e-01, -2.4024e-01, -4.6237e-01,  7.0151e-02,  8.6054e-02,\n",
       "        -4.9971e-01,  2.2543e-01,  1.2051e+00, -4.0800e-01, -4.0347e-01,\n",
       "         6.4969e-02,  2.2159e-01, -3.5907e-01, -7.2401e-02, -5.1832e-01,\n",
       "         2.8684e-01,  2.7066e-01,  3.6401e-01,  8.5634e-02, -3.3139e-01,\n",
       "         6.3360e-01,  1.7523e-01,  1.4234e-01, -2.7293e-01,  4.8939e-01,\n",
       "        -9.2916e-02,  4.7660e-01, -2.3027e-01,  2.6775e-01, -7.4336e-01,\n",
       "        -5.2749e-01,  2.4657e-01,  8.7588e-02,  1.2593e-01, -4.2729e-01,\n",
       "         5.4717e-01, -3.7064e-02,  7.9897e-02, -1.2902e-01, -3.3426e-01,\n",
       "        -1.2603e-01, -3.3981e-01,  5.6647e-01, -4.0939e-02,  2.3199e+00,\n",
       "         9.2844e-02,  5.5569e-01, -2.6277e-01, -1.6669e-01,  1.6468e-01,\n",
       "         4.3346e-01,  8.7114e-01, -5.7562e-01,  5.3735e-01, -3.7367e-01,\n",
       "         5.5586e-01, -7.5947e-04, -1.3025e-01,  1.5074e-01,  5.4900e-01,\n",
       "         5.1797e-01, -1.2908e-01, -1.0219e-01, -6.4145e-02,  3.8506e-01,\n",
       "         2.6172e-01,  2.4987e-01, -5.3404e-02, -4.2826e-01, -1.1277e-01,\n",
       "        -2.4850e-01, -9.9860e-02,  5.7598e-01,  5.0631e-01, -2.0591e-02,\n",
       "         4.2214e-01, -6.8796e-01, -3.8984e-01,  9.7404e-01,  6.0293e-01,\n",
       "         7.2883e-01,  2.0109e-01, -3.9821e-01, -1.5889e-01, -4.7531e-01,\n",
       "        -6.7037e-01,  1.1553e-01, -1.3463e-01,  4.4506e-01,  3.0875e-01,\n",
       "         1.0896e-01, -6.5494e-01,  6.6309e-03,  1.9463e-01,  6.8472e-02,\n",
       "        -6.2631e-01,  8.1563e-01,  2.9551e-01, -2.5326e-01,  7.2633e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors[glove.stoi[\"my\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74a8b3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select',\n",
       " 'product_price',\n",
       " 'from',\n",
       " 'products',\n",
       " 'where',\n",
       " 'product_id',\n",
       " 'not',\n",
       " 'in',\n",
       " '(',\n",
       " 'select',\n",
       " 'product_id',\n",
       " 'from',\n",
       " 'complaints',\n",
       " ')']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(string):\n",
    "    string = str(string)\n",
    "    string = string.replace(\"\\'\", \"\\\"\")  # ensures all string values wrapped by \"\" problem??\n",
    "    quote_idxs = [idx for idx, char in enumerate(string) if char == '\"']\n",
    "    assert len(quote_idxs) % 2 == 0, \"Unexpected quote\"\n",
    "\n",
    "    # keep string value as token\n",
    "    vals = {}\n",
    "    for i in range(len(quote_idxs)-1, -1, -2):\n",
    "        qidx1 = quote_idxs[i-1]\n",
    "        qidx2 = quote_idxs[i]\n",
    "        val = string[qidx1: qidx2+1]\n",
    "        key = \"__val_{}_{}__\".format(qidx1, qidx2)\n",
    "        string = string[:qidx1] + key + string[qidx2+1:]\n",
    "        vals[key] = val\n",
    "\n",
    "    toks = [word.lower() for word in word_tokenize(string)]\n",
    "    # replace with string value token\n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] in vals:\n",
    "            toks[i] = vals[toks[i]]\n",
    "\n",
    "    # find if there exists !=, >=, <=\n",
    "    eq_idxs = [idx for idx, tok in enumerate(toks) if tok == \"=\"]\n",
    "    eq_idxs.reverse()\n",
    "    prefix = ('!', '>', '<')\n",
    "    for eq_idx in eq_idxs:\n",
    "        pre_tok = toks[eq_idx-1]\n",
    "        if pre_tok in prefix:\n",
    "            toks = toks[:eq_idx-1] + [pre_tok + \"=\"] + toks[eq_idx+1: ]\n",
    "\n",
    "    tokens = []\n",
    "    for tok in toks:\n",
    "        if \".\" in tok and \"t\" in tok:\n",
    "            tokens.extend(tok.replace(\".\", \" . \").split())\n",
    "        else:\n",
    "            tokens.append(tok)\n",
    "    return tokens\n",
    "\n",
    "tokenize(\"select product_price from products where product_id not in ( select product_id from complaints )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c23e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
